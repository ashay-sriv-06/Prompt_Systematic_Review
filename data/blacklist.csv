Title, Link, Reason
A Brief History of Prompt: Leveraging Language Models., https://arxiv.org/abs/2310.04438, AI Generated
Hydrogen-rich supernovae beyond the neutrino-driven core-collapse paradigm,,About Space not Prompting
Few-Shot Learning with Localization in Realistic Settings,,not related to prompting
Cross-Lingual Alignment of Contextual Word Embeddings with Applications to Zero-shot Dependency Parsing,, no Prompting
ANALOGY-FORMING TRANSFORMERS FOR FEW-SHOT 3D PARSING,, no prompting
GENERAL-PURPOSE IN-CONTEXT LEARNING BY META-LEARNING TRANSFORMERS,, no prompting
A Survey of Deep Learning for Low-Shot Object Detection,, no prompting
Few-shot Class-incremental Learning: A Survey,, no prompting
Balanced and Explainable Social Media Analysis for Public Health with  Large Language Models,,uses BERT
QUERY-DEPENDENT PROMPT EVALUATION AND OPTI- MIZATION WITH OFFLINE INVERSE RL,,more about deep RL than prompting
DeltaEdit: Exploring Text-free Training for Text-Driven Image  Manipulation,,too training focused
Deep Language Networks: Joint Prompt Training of Stacked LLMs using  Variational Inference,, too training focused
Unnatural language processing: How do language models handle  machine-generated prompts?,, too training focused
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained  Language Models,, cloze focused
Task-driven Prompt Evolution for Foundation Models,, training related
Diversity-Aware Meta Visual Prompting,, training focused
DRPT: Disentangled and Recurrent Prompt Tuning for Compositional  Zero-Shot Learning,, tuning
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided  Image Editing,, training focused
InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image,, not really about prompting
What Changes Can Large-scale Language Models Bring? Intensive Study on  HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,, about a model not prompts
MLLM-DataEngine: An Iterative Refinement Approach for MLLM,,soft prompting
UNLEASHING THE POWER OF PRE-TRAINED LANGUAGE MODELS FOR OFFLINE REINFORCEMENT LEARNING,, out-of-scope
ExPT: Synthetic Pretraining for Few-Shot Experimental Design,, no prompting
Improving Input-label Mapping with Demonstration Replay for In-context Learning,, out-of-domain
APOLLO: ZERO-SHOT MULTIMODAL REASONING WITH MULTIPLE EXPERTS, 2310.18369v1.pdf, Lower-Level Transformer Modification - Not Prompting
Few-Shot Learning with Siamese Networks and Label Tuning,, no prompting
MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text  Classification,, no prompting
Zero and Few-shot Learning for Author Profiling,, about models not prompting
"Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong  Few-shot Learners", http://arxiv.org/pdf/2303.02151v1.pdf, training
Gradient-Regulated Meta-Prompt Learning for Generalizable  Vision-Language Models, http://arxiv.org/pdf/2303.06571v2.pdf, soft prompting
Decomposed Prototype Learning for Few-Shot Scene Graph Generation,http://arxiv.org/pdf/2303.10863v1.pdf, continuous prompts
Supervised Masked Knowledge Distillation for Few-Shot Transformers,, no prompting
"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with  Text", http://arxiv.org/pdf/2303.15466v2.pdf, no prompting
A Survey on Few-Shot Class-Incremental Learning,http://arxiv.org/pdf/2304.06939v3.pdf, no prompting
Unified Quantum State Tomography and Hamiltonian Learning Using  Transformer Models: A Language-Translation-Like Approach for Quantum Systems, http://arxiv.org/pdf/2304.08130v2.pdf, no prompting
Analogy-Forming Transformers for Few-Shot 3D Parsing, http://arxiv.org/pdf/2304.12010v1.pdf, no prompting
PointGPT: Auto-regressively Generative Pre-training from Point Clouds, http://arxiv.org/pdf/2305.11487v2.pdf, continuous prompts
A Survey of Diffusion Models in Natural Language Processing,http://arxiv.org/pdf/2305.14671v2.pdf, no prompting
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning, http://arxiv.org/pdf/2306.07967v2.pdf, tuning
ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided  Diffusion, http://arxiv.org/pdf/2306.14770v2.pdf, no prompting
Effective Transfer of Pretrained Large Visual Model for Fabric Defect  Segmentation via Specifc Knowledge Injection, http://arxiv.org/pdf/2306.16186v1.pdf, no prompting
Meta-training with Demonstration Retrieval for Efficient Few-shot  Learning, http://arxiv.org/pdf/2307.00119v1.pdf, cloze prompting
TablEye: Seeing small Tables through the Lens of Images, http://arxiv.org/pdf/2307.02491v1.pdf, no prompting
Identifying Misinformation on YouTube through Transcript Contextual  Analysis with Transformer Models, http://arxiv.org/pdf/2307.12155v1.pdf, no prompting
Link-Context Learning for Multimodal LLMs, http://arxiv.org/pdf/2308.07891v1.pdf, no prompting
Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via  Training-free Networks, http://arxiv.org/pdf/2308.12961v1.pdf, no prompting
TransPrompt v2: A Transferable Prompting Framework for Cross-task Text  Classification, http://arxiv.org/pdf/2308.15010v1.pdf, soft prompting
Self-Sampling Meta SAM: Enhancing Few-shot Medical Image Segmentation  with Meta-Learning, http://arxiv.org/pdf/2308.16466v3.pdf, training
Prompt-based Node Feature Extractor for Few-shot Learning on  Text-Attributed Graphs, http://arxiv.org/pdf/2309.02848v1.pdf, cloze prompts
Cross-Image Context Matters for Bongard Problems, http://arxiv.org/pdf/2309.03468v1.pdf, no prompting
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning, http://arxiv.org/pdf/2309.05173v2.pdf, tuning
GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection, http://arxiv.org/pdf/2309.05953v1.pdf, cloze prompting
SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient  Channels, http://arxiv.org/pdf/2309.08513v2.pdf, tuning
PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven  Perturbed Gradient Descent, http://arxiv.org/pdf/2310.17588v1.pdf, no prompting
Unleashing the Power of Pre-trained Language Models for Offline  Reinforcement Learning, http://arxiv.org/pdf/2310.20587v3.pdf, no prompting
On Task-personalized Multimodal Few-shot Learning for Visually-rich  Document Entity Retrieval, http://arxiv.org/pdf/2311.00693v1.pdf, no prompting
Robust Fine-Tuning of Vision-Language Models for Domain Generalization, http://arxiv.org/pdf/2311.02236v1.pdf, no prompting
Lesion2Vec: Deep Metric Learning for Few-Shot Multiple Lesions  Recognition in Wireless Capsule Endoscopy Video, http://arxiv.org/pdf/2101.04240v2.pdf, no prompting
Unsupervised Law Article Mining based on Deep Pre-Trained Language  Representation Models with Application to the Italian Civil Code, http://arxiv.org/pdf/2112.03033v1.pdf, no prompting
"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A  Large-Scale Generative Language Model", http://arxiv.org/pdf/2201.11990v3.pdf, training
Data Distributional Properties Drive Emergent In-Context Learning in  Transformers, http://arxiv.org/pdf/2205.05055v6.pdf, no prompting
Hungry Hungry Hippos: Towards Language Modeling with State Space Models, http://arxiv.org/pdf/2212.14052v3.pdf, no prompting
CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP, http://arxiv.org/pdf/2301.04926v2.pdf, cloze prompting
Learning to detect an animal sound from five examples, http://arxiv.org/pdf/2305.13210v1.pdf, no prompting
The Rise of AI Language Pathologists: Exploring Two-level Prompt  Learning for Few-shot Weakly-supervised Whole Slide Image Classification, http://arxiv.org/pdf/2305.17891v1.pdf, training
Language Models are Few-Shot Learners, http://arxiv.org/pdf/2005.14165v4.pdf, training
When Prompt-based Incremental Learning Does Not Meet Strong Pretraining, http://arxiv.org/pdf/2308.10445v1.pdf, training
"Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender  Bias", http://arxiv.org/pdf/2206.09860v1.pdf, MLMs and cloze prompting
PromptAttack: Prompt-based Attack for Language Models via Gradient  Search, http://arxiv.org/pdf/2209.01882v1.pdf, cloze prompting
Can Language Models Be Specific? How?, http://arxiv.org/pdf/2210.05159v2.pdf, cloze prompting
Multilingual Relation Classification via Efficient and Effective  Prompting, http://arxiv.org/pdf/2210.13838v2.pdf, soft prompting
SPE: Symmetrical Prompt Enhancement for Fact Probing, http://arxiv.org/pdf/2211.07078v1.pdf, soft prompting
Evaluating the Robustness of Discrete Prompts, http://arxiv.org/pdf/2302.05619v1.pdf, cloze prompting
Syntax-aware Hybrid prompt model for Few-shot multi-modal sentiment  analysis, http://arxiv.org/pdf/2306.01312v2.pdf, soft and cloze prompting
Unified Multimodal Pre-training and Prompt-based Tuning for  Vision-Language Understanding and Generation, http://arxiv.org/pdf/2112.05587v2.pdf, MLMs and cloze prompting
Learning to Transfer Prompts for Text Generation, http://arxiv.org/pdf/2205.01543v2.pdf, soft prompting
Towards Realistic Low-resource Relation Extraction: A Benchmark with  Empirical Baseline Study, http://arxiv.org/pdf/2210.10678v3.pdf, tuning and cloze prompting
PromptFusion: Decoupling Stability and Plasticity for Continual Learning, http://arxiv.org/pdf/2303.07223v1.pdf, tuning
Are Prompt-based Models Clueless?, http://arxiv.org/pdf/2205.09295v2.pdf, cloze prompting
Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning, http://arxiv.org/pdf/2109.04144v1.pdf, tuning
P4E: Few-Shot Event Detection as Prompt-Guided Identification and  Localization, http://arxiv.org/pdf/2202.07615v3.pdf, cloze prompting
PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained  Image-Language Models, http://arxiv.org/pdf/2212.01558v2.pdf, tuning
SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly  Generating Predictions and Natural Language Explanations, http://arxiv.org/pdf/2305.13235v2.pdf, training and tuning
Large Language Model Distillation Doesn't Need a Teacher, http://arxiv.org/pdf/2305.14864v1.pdf, training
MultiQG-TI: Towards Question Generation from Multi-modal Sources, http://arxiv.org/pdf/2307.04643v1.pdf, no prompting
Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?, http://arxiv.org/pdf/2307.11978v1.pdf, tuning
Low-Parameter Federated Learning with Large Language Models, http://arxiv.org/pdf/2307.13896v1.pdf, tuning and MLM
OLaLa: Ontology Matching with Large Language Models, http://arxiv.org/pdf/2311.03837v1.pdf, uses BERT no specified prefix prompting
Cross-Lingual Supervision improves Large Language Models Pre-training, http://arxiv.org/pdf/2305.11778v1.pdf, training focused
ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist  Examination,http://arxiv.org/pdf/2305.12945v2.pdf, training focused
Adapting Language Models to Compress Contexts, http://arxiv.org/pdf/2305.14788v2.pdf, soft prompting
A Mechanism for Sample-Efficient In-Context Learning for Sparse  Retrieval Tasks, http://arxiv.org/pdf/2305.17040v1.pdf, more about LM interpretability than prompting
Large Language Models Are Partially Primed in Pronoun Interpretation, http://arxiv.org/pdf/2305.16917v1.pdf, uses in-context learning but is not about prompting methods
A Mechanism for Sample-Efficient In-Context Learning for Sparse  Retrieval Tasks,http://arxiv.org/pdf/2305.17040v1.pdf, aims to explain in-context learning instead of proposing methods
Contextual Vision Transformers for Robust Representation Learning,http://arxiv.org/pdf/2305.19402v2.pdf, not about prefix prompting
Self-Verification Improves Few-Shot Clinical Information Extraction, http://arxiv.org/pdf/2306.00024v1.pdf, is about verifying output not modifying input
Measuring and Modifying Factual Knowledge in Large Language Models,http://arxiv.org/pdf/2306.06264v1.pdf, mentions in context learning but it is not the focus
A Survey on Multimodal Large Language Models,http://arxiv.org/pdf/2306.13549v1.pdf, not focused on prompting
Potential Benefits of Employing Large Language Models in Research in  Moral Education and Development,http://arxiv.org/pdf/2306.13805v2.pdf, not particuyarly about prompting
Assessing the efficacy of large language models in generating accurate  teacher responses,http://arxiv.org/pdf/2307.04274v1.pdf, does not focus on prompting methods
Unsupervised Calibration through Prior Adaptation for Text  Classification using Large Language Models,http://arxiv.org/pdf/2307.06713v3.pdf, does not focus on prompting methods
Baby's CoThought: Leveraging Large Language Models for Enhanced  Reasoning in Compact Models,http://arxiv.org/pdf/2308.01684v2.pdf, focuses on training other models
Diffusion Language Models Can Perform Many Tasks with Scaling and  Instruction-Finetuning,http://arxiv.org/pdf/2308.12219v2.pdf, focuses on training
Large Language Model as Autonomous Decision Maker,http://arxiv.org/pdf/2308.12519v1.pdf, not about prompting methods
Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer,http://arxiv.org/pdf/2309.07566v1.pdf, speech to speech translation
Language Modeling Is Compression,http://arxiv.org/pdf/2309.10668v1.pdf, more about explaining in-context learning than proposing a method
Text Data Augmentation in Low-Resource Settings via Fine-Tuning of Large  Language Models,http://arxiv.org/pdf/2310.01119v1.pdf, focuses on training
Humans and language models diverge when predicting repeating text,http://arxiv.org/pdf/2310.06408v2.pdf, focuses on evaluating humans and comparing to prompting method
AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents,http://arxiv.org/pdf/2310.09971v2.pdf, not about LMs; this is an RL paper
Meta- (out-of-context) learning in neural networks,,http://arxiv.org/pdf/2310.15047v2.pdf, evaluates in-context learning but is not based on it
Towards Training-free Open-world Segmentation via Image Prompting Foundation Models,http://arxiv.org/pdf/2310.10912v1.pdf,image segmentation,,
Videoprompter: an ensemble of foundational models for zero-shot video understanding,http://arxiv.org/pdf/2310.15324v1.pdf,"video understanding, different domain",,
Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting,http://arxiv.org/pdf/2310.16523v1.pdf,"model representation, not prompting",,
The Power of Large Language Models for Wireless Communication System Development: A Case Study on FPGA Platforms,http://arxiv.org/pdf/2307.07319v4.pdf,not prompting,,
Large Language Models Enable Few-Shot Clustering,http://arxiv.org/pdf/2307.00524v1.pdf,"few-shot clustering, not prompting",,
Universal Fuzzing via Large Language Models,http://arxiv.org/pdf/2308.04748v1.pdf,does not use hard-prefix prompts,,
Training-free Open-world Segmentation via Image Prompting Foundation Models,,image segmentation,,
FIRE: Food Image to REcipe generation,http://arxiv.org/pdf/2308.14391v1.pdf,image to text translation,,
Large language models can accurately predict searcher preferences,http://arxiv.org/pdf/2309.10621v1.pdf,does not use hard-prefix prompts,,
Understanding In-Context Learning from Repetitions,http://arxiv.org/pdf/2310.00297v2.pdf,"focus is on effects of repetition in in-context learning, not prompting",,
Small Language Models Fine-tuned to Coordinate Larger Language Models  improve Complex Reasoning,http://arxiv.org/pdf/2310.18338v1.pdf,"focus on fine-tuning, not hard-prefix prompting",,
Revisiting Large Language Models as Zero-shot Relation Extractors,http://arxiv.org/pdf/2310.05028v3.pdf,zero-shot learning for relation extraction,,
Towards Training-free Open-world Segmentation via Image Prompting  Foundation Models,http://arxiv.org/pdf/2310.10912v1.pdf,image segmentation,,
Improving Diversity of Demographic Representation in Large Language  Models via Collective-Critiques and Self-Voting,http://arxiv.org/pdf/2310.16523v1.pdf,demographic representation,,
Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented  Large Language Models,http://arxiv.org/pdf/2302.05578v2.pdf,RAG,,
LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain  Conversations with Large Language Models,http://arxiv.org/pdf/2305.13711v1.pdf,eval of LLMs,,
Robot Task Planning Based on Large Language Model Representing Knowledge  with Directed Graph Structures,http://arxiv.org/pdf/2306.05171v1.pdf,knowledge representation,,
OptiMUS: Optimization Modeling Using MIP Solvers and large language  models,http://arxiv.org/pdf/2310.06116v2.pdf,"different approach, MIP solvers",,
PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers'  Workflows,http://arxiv.org/pdf/2310.15435v1.pdf,focus on UI,,
A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event  Extraction,http://arxiv.org/pdf/2305.15051v1.pdf,"monte carlo methods, not prompting",,
Large language models can accurately predict searcher preferences,http://arxiv.org/pdf/2309.10621v1.pdf,"prediction capabilities, not prompting",,
Fine-tune Language Models to Approximate Unbiased In-context Learning,http://arxiv.org/pdf/2310.03331v1.pdf,fine-tuning,,
On the Compositional Generalization Gap of In-Context Learning,http://arxiv.org/pdf/2211.08473v1.pdf,"compositional generalization, not hard-prefix prompting",,
Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and  Evaluation,http://arxiv.org/pdf/2305.16938v2.pdf,no hard-prefix prompting,,
