Title, Link, Reason
A Brief History of Prompt: Leveraging Language Models., https://arxiv.org/abs/2310.04438, AI Generated
Hydrogen-rich supernovae beyond the neutrino-driven core-collapse paradigm,,About Space not Prompting
Few-Shot Learning with Localization in Realistic Settings,,not related to prompting
Cross-Lingual Alignment of Contextual Word Embeddings with Applications to Zero-shot Dependency Parsing,, no Prompting
ANALOGY-FORMING TRANSFORMERS FOR FEW-SHOT 3D PARSING,, no prompting
GENERAL-PURPOSE IN-CONTEXT LEARNING BY META-LEARNING TRANSFORMERS,, no prompting
A Survey of Deep Learning for Low-Shot Object Detection,, no prompting
Few-shot Class-incremental Learning: A Survey,, no prompting
Balanced and Explainable Social Media Analysis for Public Health with  Large Language Models,,uses BERT
QUERY-DEPENDENT PROMPT EVALUATION AND OPTI- MIZATION WITH OFFLINE INVERSE RL,,more about deep RL than prompting
DeltaEdit: Exploring Text-free Training for Text-Driven Image  Manipulation,,too training focused
Deep Language Networks: Joint Prompt Training of Stacked LLMs using  Variational Inference,, too training focused
Unnatural language processing: How do language models handle  machine-generated prompts?,, too training focused
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained  Language Models,, cloze focused
Task-driven Prompt Evolution for Foundation Models,, training related
Diversity-Aware Meta Visual Prompting,, training focused
DRPT: Disentangled and Recurrent Prompt Tuning for Compositional  Zero-Shot Learning,, tuning
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided  Image Editing,, training focused
InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image,, not really about prompting
What Changes Can Large-scale Language Models Bring? Intensive Study on  HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,, about a model not prompts
MLLM-DataEngine: An Iterative Refinement Approach for MLLM,,soft prompting
UNLEASHING THE POWER OF PRE-TRAINED LANGUAGE MODELS FOR OFFLINE REINFORCEMENT LEARNING,, out-of-scope
ExPT: Synthetic Pretraining for Few-Shot Experimental Design,, no prompting
Improving Input-label Mapping with Demonstration Replay for In-context Learning,, out-of-domain
APOLLO: ZERO-SHOT MULTIMODAL REASONING WITH MULTIPLE EXPERTS, 2310.18369v1.pdf, Lower-Level Transformer Modification - Not Prompting
Few-Shot Learning with Siamese Networks and Label Tuning,, no prompting
MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text  Classification,, no prompting
Zero and Few-shot Learning for Author Profiling,, about models not prompting
"Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong  Few-shot Learners",,training focused
Knowledge-augmented Few-shot Visual Relation Detection,, largely about visual relation detection
Gradient-Regulated Meta-Prompt Learning for Generalizable  Vision-Language Models,, soft prompting
Decomposed Prototype Learning for Few-Shot Scene Graph Generation,, largely about decomposed prototyle learning
Supervised Masked Knowledge Distillation for Few-Shot Transformers,, no prompting
"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with  Text",, no prompting
A Survey on Few-Shot Class-Incremental Learning,, no prompting
Unified Quantum State Tomography and Hamiltonian Learning Using  Transformer Models: A Language-Translation-Like Approach for Quantum Systems,, no prompting
Analogy-Forming Transformers for Few-Shot 3D Parsing,, no prompting
PointGPT: Auto-regressively Generative Pre-training from Point Clouds,, not relevant to hard prefix prompting
A Survey of Diffusion Models in Natural Language Processing,, no prompting




