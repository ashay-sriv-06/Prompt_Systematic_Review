Title, Link, Reason
A Brief History of Prompt: Leveraging Language Models., https://arxiv.org/abs/2310.04438, AI Generated
Hydrogen-rich supernovae beyond the neutrino-driven core-collapse paradigm,,About Space not Prompting
Few-Shot Learning with Localization in Realistic Settings,,not related to prompting
Cross-Lingual Alignment of Contextual Word Embeddings,, with Applications to Zero-shot Dependency Parsing,, no Prompting
ANALOGY-FORMING TRANSFORMERS FOR FEW-SHOT 3D PARSING,, no prompting
GENERAL-PURPOSE IN-CONTEXT LEARNING BY META-LEARNING TRANSFORMERS,, no prompting
A Survey of Deep Learning for Low-Shot Object Detection,, no prompting
Few-shot Class-incremental Learning: A Survey,, no prompting
Balanced and Explainable Social Media Analysis for Public Health with  Large Language Models,,uses BERT
QUERY-DEPENDENT PROMPT EVALUATION AND OPTI- MIZATION WITH OFFLINE INVERSE RL,,more about deep RL than prompting
DeltaEdit: Exploring Text-free Training for Text-Driven Image  Manipulation,,too training focused
Deep Language Networks: Joint Prompt Training of Stacked LLMs using  Variational Inference,, too training focused
Unnatural language processing: How do language models handle  machine-generated prompts?,, too training focused
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained  Language Models,, cloze focused
Task-driven Prompt Evolution for Foundation Models,, training related
Diversity-Aware Meta Visual Prompting,, training focused
DRPT: Disentangled and Recurrent Prompt Tuning for Compositional  Zero-Shot Learning,, tuning
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided  Image Editing,, training focused
InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image,, not really about prompting
What Changes Can Large-scale Language Models Bring? Intensive Study on  HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,, about a model not prompts
MLLM-DataEngine: An Iterative Refinement Approach for MLLM,,soft prompting
UNLEASHING THE POWER OF PRE-TRAINED LANGUAGE MODELS FOR OFFLINE REINFORCEMENT LEARNING,, out-of-scope
ExPT: Synthetic Pretraining for Few-Shot Experimental Design,, no prompting
Improving Input-label Mapping with Demonstration Replay for In-context Learning,, out-of-domain
APOLLO: ZERO-SHOT MULTIMODAL REASONING WITH MULTIPLE EXPERTS, 2310.18369v1.pdf, Lower-Level Transformer Modification - Not Prompting
Few-Shot Learning with Siamese Networks and Label Tuning,, no prompting
MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text  Classification,, no prompting
Zero and Few-shot Learning for Author Profiling,, about models not prompting
Towards Training-free Open-world Segmentation via Image Prompting Foundation Models,http://arxiv.org/pdf/2310.10912v1.pdf,image segmentation,,
Videoprompter: an ensemble of foundational models for zero-shot video understanding,http://arxiv.org/pdf/2310.15324v1.pdf,"video understanding, different domain",,
Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting,http://arxiv.org/pdf/2310.16523v1.pdf,"model representation, not prompting",,
The Power of Large Language Models for Wireless Communication System Development: A Case Study on FPGA Platforms,http://arxiv.org/pdf/2307.07319v4.pdf,not prompting,,
Large Language Models Enable Few-Shot Clustering,http://arxiv.org/pdf/2307.00524v1.pdf,"few-shot clustering, not prompting",,
Universal Fuzzing via Large Language Models,http://arxiv.org/pdf/2308.04748v1.pdf,does not use hard-prefix prompts,,
Training-free Open-world Segmentation via Image Prompting Foundation Models,,image segmentation,,
FIRE: Food Image to REcipe generation,http://arxiv.org/pdf/2308.14391v1.pdf,image to text translation,,
Large language models can accurately predict searcher preferences,http://arxiv.org/pdf/2309.10621v1.pdf,does not use hard-prefix prompts,,
Understanding In-Context Learning from Repetitions,http://arxiv.org/pdf/2310.00297v2.pdf,"focus is on effects of repetition in in-context learning, not prompting",,
Small Language Models Fine-tuned to Coordinate Larger Language Models  improve Complex Reasoning,http://arxiv.org/pdf/2310.18338v1.pdf,"focus on fine-tuning, not hard-prefix prompting",,
Revisiting Large Language Models as Zero-shot Relation Extractors,http://arxiv.org/pdf/2310.05028v3.pdf,zero-shot learning for relation extraction,,
Towards Training-free Open-world Segmentation via Image Prompting  Foundation Models,http://arxiv.org/pdf/2310.10912v1.pdf,image segmentation,,
Improving Diversity of Demographic Representation in Large Language  Models via Collective-Critiques and Self-Voting,http://arxiv.org/pdf/2310.16523v1.pdf,demographic representation,,
Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented  Large Language Models,http://arxiv.org/pdf/2302.05578v2.pdf,RAG,,
LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain  Conversations with Large Language Models,http://arxiv.org/pdf/2305.13711v1.pdf,eval of LLMs,,
Robot Task Planning Based on Large Language Model Representing Knowledge  with Directed Graph Structures,http://arxiv.org/pdf/2306.05171v1.pdf,knowledge representation,,
OptiMUS: Optimization Modeling Using MIP Solvers and large language  models,http://arxiv.org/pdf/2310.06116v2.pdf,"different approach, MIP solvers",,
PromptInfuser: How Tightly Coupling AI and UI Design Impacts Designers'  Workflows,http://arxiv.org/pdf/2310.15435v1.pdf,focus on UI,,
A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event  Extraction,http://arxiv.org/pdf/2305.15051v1.pdf,"monte carlo methods, not prompting",,
Large language models can accurately predict searcher preferences,http://arxiv.org/pdf/2309.10621v1.pdf,"prediction capabilities, not prompting",,
Fine-tune Language Models to Approximate Unbiased In-context Learning,http://arxiv.org/pdf/2310.03331v1.pdf,fine-tuning,,
On the Compositional Generalization Gap of In-Context Learning,http://arxiv.org/pdf/2211.08473v1.pdf,"compositional generalization, not hard-prefix prompting",,
Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and  Evaluation,http://arxiv.org/pdf/2305.16938v2.pdf,no hard-prefix prompting,,
