Title, Link, Reason
A Brief History of Prompt: Leveraging Language Models., https://arxiv.org/abs/2310.04438, AI Generated
Hydrogen-rich supernovae beyond the neutrino-driven core-collapse paradigm,,About Space not Prompting
Few-Shot Learning with Localization in Realistic Settings,,not related to prompting
Cross-Lingual Alignment of Contextual Word Embeddings,, with Applications to Zero-shot Dependency Parsing,, no Prompting
ANALOGY-FORMING TRANSFORMERS FOR FEW-SHOT 3D PARSING,, no prompting
GENERAL-PURPOSE IN-CONTEXT LEARNING BY META-LEARNING TRANSFORMERS,, no prompting
A Survey of Deep Learning for Low-Shot Object Detection,, no prompting
Few-shot Class-incremental Learning: A Survey,, no prompting
Balanced and Explainable Social Media Analysis for Public Health with  Large Language Models,,uses BERT
QUERY-DEPENDENT PROMPT EVALUATION AND OPTI- MIZATION WITH OFFLINE INVERSE RL,,more about deep RL than prompting
DeltaEdit: Exploring Text-free Training for Text-Driven Image  Manipulation,,too training focused
Deep Language Networks: Joint Prompt Training of Stacked LLMs using  Variational Inference,, too training focused
Unnatural language processing: How do language models handle  machine-generated prompts?,, too training focused
Give Me the Facts! A Survey on Factual Knowledge Probing in Pre-trained  Language Models,, cloze focused
Task-driven Prompt Evolution for Foundation Models,, training related
Diversity-Aware Meta Visual Prompting,, training focused
DRPT: Disentangled and Recurrent Prompt Tuning for Compositional  Zero-Shot Learning,, tuning
DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided  Image Editing,, training focused
InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image,, not really about prompting
What Changes Can Large-scale Language Models Bring? Intensive Study on  HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,, about a model not prompts
MLLM-DataEngine: An Iterative Refinement Approach for MLLM,,soft prompting
UNLEASHING THE POWER OF PRE-TRAINED LANGUAGE MODELS FOR OFFLINE REINFORCEMENT LEARNING,, out-of-scope
ExPT: Synthetic Pretraining for Few-Shot Experimental Design,, no prompting
Improving Input-label Mapping with Demonstration Replay for In-context Learning,, out-of-domain
APOLLO: ZERO-SHOT MULTIMODAL REASONING WITH MULTIPLE EXPERTS, 2310.18369v1.pdf, Lower-Level Transformer Modification - Not Prompting
Few-Shot Learning with Siamese Networks and Label Tuning,, no prompting
MGIMN: Multi-Grained Interactive Matching Network for Few-shot Text  Classification,, no prompting
Zero and Few-shot Learning for Author Profiling,, about models not prompting
